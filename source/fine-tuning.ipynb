{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12543401,"sourceType":"datasetVersion","datasetId":7919180}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:06:05.318852Z","iopub.execute_input":"2025-08-24T09:06:05.319118Z","iopub.status.idle":"2025-08-24T09:06:08.750465Z","shell.execute_reply.started":"2025-08-24T09:06:05.319088Z","shell.execute_reply":"2025-08-24T09:06:08.749691Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    DistilBertTokenizer, DistilBertForSequenceClassification,\n    RobertaTokenizer, RobertaForSequenceClassification,\n    Trainer, TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport evaluate\nimport os\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:06:12.424766Z","iopub.execute_input":"2025-08-24T09:06:12.425042Z","iopub.status.idle":"2025-08-24T09:06:21.619013Z","shell.execute_reply.started":"2025-08-24T09:06:12.425015Z","shell.execute_reply":"2025-08-24T09:06:21.618432Z"}},"outputs":[{"name":"stderr","text":"2025-08-24 09:06:18.211546: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756026378.235103     168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756026378.242060     168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"# Load preprocessed dataset \ndata = pd.read_csv('/kaggle/input/pretrained-data/preprocessed_results.csv')\ndata['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:06:21.620378Z","iopub.execute_input":"2025-08-24T09:06:21.621285Z","iopub.status.idle":"2025-08-24T09:06:22.093524Z","shell.execute_reply.started":"2025-08-24T09:06:21.621262Z","shell.execute_reply":"2025-08-24T09:06:22.092717Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Split the dataset into training, validation, and test sets\ntrain_data, temp_data = train_test_split(data, test_size=0.3, random_state=42, stratify=data['sentiment'])\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['sentiment'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:06:22.094389Z","iopub.execute_input":"2025-08-24T09:06:22.094632Z","iopub.status.idle":"2025-08-24T09:06:22.128054Z","shell.execute_reply.started":"2025-08-24T09:06:22.094605Z","shell.execute_reply":"2025-08-24T09:06:22.127527Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Save datasets to CSV files\nos.makedirs('/kaggle/working/datasets', exist_ok=True)\ntrain_data.to_csv('/kaggle/working/datasets/train_data.csv', index=False)\nval_data.to_csv('/kaggle/working/datasets/val_data.csv', index=False)\ntest_data.to_csv('/kaggle/working/datasets/test_data.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:06:22.129432Z","iopub.execute_input":"2025-08-24T09:06:22.129906Z","iopub.status.idle":"2025-08-24T09:06:23.213081Z","shell.execute_reply.started":"2025-08-24T09:06:22.129885Z","shell.execute_reply":"2025-08-24T09:06:23.212334Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Convert to DatasetDict\nraw_datasets = DatasetDict({\n    'train': Dataset.from_pandas(train_data[['preprocessed_tokens', 'sentiment']].rename(columns={'preprocessed_tokens': 'text', 'sentiment': 'labels'})),\n    'valid': Dataset.from_pandas(val_data[['preprocessed_tokens', 'sentiment']].rename(columns={'preprocessed_tokens': 'text', 'sentiment': 'labels'})),\n    'test': Dataset.from_pandas(test_data[['preprocessed_tokens', 'sentiment']].rename(columns={'preprocessed_tokens': 'text', 'sentiment': 'labels'})),\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:06:23.613627Z","iopub.execute_input":"2025-08-24T09:06:23.613904Z","iopub.status.idle":"2025-08-24T09:06:23.949989Z","shell.execute_reply.started":"2025-08-24T09:06:23.613883Z","shell.execute_reply":"2025-08-24T09:06:23.949393Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Define functions ","metadata":{}},{"cell_type":"code","source":"# Function to compute metrics\ndef compute_metrics(eval_pred):\n    metric_acc = evaluate.load(\"accuracy\")\n    metric_f1 = evaluate.load(\"f1\")\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    acc = metric_acc.compute(predictions=predictions, references=labels)\n    f1 = metric_f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:06:26.478591Z","iopub.execute_input":"2025-08-24T09:06:26.478871Z","iopub.status.idle":"2025-08-24T09:06:26.483745Z","shell.execute_reply.started":"2025-08-24T09:06:26.478850Z","shell.execute_reply":"2025-08-24T09:06:26.482886Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Function to fine-tune models\ndef fine_tune_model(model_name, tokenizer_class, model_class, output_dir, tokenized_datasets):\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=256)\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=3,\n        eval_strategy=\"epoch\",\n        weight_decay=5e-4,\n        optim=\"adamw_torch\",\n        learning_rate=5e-5,\n        save_strategy=\"no\",\n        fp16=True,\n        push_to_hub=False,\n        report_to=\"none\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets['train'],\n        eval_dataset=tokenized_datasets['valid'],\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n\n    # Fine-tune the model\n    trainer.train()\n\n    # Evaluate on the validation set\n    val_results = trainer.evaluate()\n    print(f\"Evaluation results of the fine-tuned model ({model_name}) on the validation set: {val_results}\")\n\n    # Evaluate on the test set\n    test_results = trainer.evaluate(tokenized_datasets['test'])\n    print(f\"Evaluation results of the fine-tuned model ({model_name}) on the test set: {test_results}\")\n\n    # Save the fine-tuned model and tokenizer\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Fine-tuned model {model_name} has been saved to {output_dir}\")\n\n    # Predictions for generating a classification report\n    y_pred = trainer.predict(tokenized_datasets['test']).predictions\n    y_pred = np.argmax(y_pred, axis=-1)\n    y_true = tokenized_datasets['test']['labels']\n    print(f\"\\nClassification Report (fine-tuned {model_name}):\")\n    print(classification_report(y_true, y_pred, digits=3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:06:28.587921Z","iopub.execute_input":"2025-08-24T09:06:28.588246Z","iopub.status.idle":"2025-08-24T09:06:28.594836Z","shell.execute_reply.started":"2025-08-24T09:06:28.588219Z","shell.execute_reply":"2025-08-24T09:06:28.594160Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Function to tokenize data\ndef tokenize_function(examples, tokenizer):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512, return_token_type_ids=False)\n\n# Config DistilBERT model\ndistilbert_config = {\n    'model_name': \"distilbert-base-uncased-finetuned-sst-2-english\",\n    'tokenizer_class': DistilBertTokenizer,\n    'model_class': DistilBertForSequenceClassification,\n    'output_dir': \"/kaggle/working/distilbert_finetuned\"\n}\n\n# Config RoBERTa model\ntwitter_roberta_config = {\n    'model_name': \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    'tokenizer_class': AutoTokenizer,\n    'model_class': AutoModelForSequenceClassification,\n    'output_dir': \"/kaggle/working/twitter_roberta_finetuned\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:06:32.778494Z","iopub.execute_input":"2025-08-24T09:06:32.778741Z","iopub.status.idle":"2025-08-24T09:06:32.783415Z","shell.execute_reply.started":"2025-08-24T09:06:32.778725Z","shell.execute_reply":"2025-08-24T09:06:32.782816Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Fine-tune models","metadata":{}},{"cell_type":"markdown","source":"## 1. DistilBERT","metadata":{}},{"cell_type":"code","source":"print(\"\\nDistilBERT\")\n# Tokenize data for DistilBERT\ndistilbert_tokenizer = distilbert_config['tokenizer_class'].from_pretrained(distilbert_config['model_name'])\ndistilbert_tokenized_datasets = raw_datasets.map(lambda x: tokenize_function(x, distilbert_tokenizer), batched=True)\ndistilbert_tokenized_datasets = distilbert_tokenized_datasets.remove_columns(['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:06:36.453231Z","iopub.execute_input":"2025-08-24T09:06:36.453516Z","iopub.status.idle":"2025-08-24T09:08:28.320876Z","shell.execute_reply.started":"2025-08-24T09:06:36.453500Z","shell.execute_reply":"2025-08-24T09:08:28.320314Z"}},"outputs":[{"name":"stdout","text":"\nDistilBERT\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51fcb79fe3304d95a3e5bd07d316cdff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fee82e17741448daeb6431f1bc0af35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe7484c7010540f6bd89931a42708405"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c62ac01ab84370a9da8a612f3ea2b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99cf50b04c04020a2b716149c43242c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6370f31912834a5eb1151f7de3fa1dfd"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Fine-tuning DistilBERT\nfine_tune_model(\n    distilbert_config['model_name'],\n    distilbert_config['tokenizer_class'],\n    distilbert_config['model_class'],\n    distilbert_config['output_dir'],\n    distilbert_tokenized_datasets\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:08:28.322120Z","iopub.execute_input":"2025-08-24T09:08:28.322369Z","iopub.status.idle":"2025-08-24T10:01:37.182925Z","shell.execute_reply.started":"2025-08-24T09:08:28.322352Z","shell.execute_reply":"2025-08-24T10:01:37.182131Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ff51843f0274208ac1d598561808257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13125' max='13125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13125/13125 49:58, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.306800</td>\n      <td>0.312422</td>\n      <td>0.893600</td>\n      <td>0.893575</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.203000</td>\n      <td>0.398923</td>\n      <td>0.904133</td>\n      <td>0.904125</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.099300</td>\n      <td>0.463527</td>\n      <td>0.908000</td>\n      <td>0.907996</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df950fdc788483298e4d8ad71bd5063"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d085507a24b142fb8f110980d5a82305"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Evaluation results of the fine-tuned model (distilbert-base-uncased-finetuned-sst-2-english) on the validation set: {'eval_loss': 0.46352747082710266, 'eval_accuracy': 0.908, 'eval_f1': 0.9079955772451711, 'eval_runtime': 61.3986, 'eval_samples_per_second': 122.153, 'eval_steps_per_second': 15.277, 'epoch': 3.0}\nEvaluation results of the fine-tuned model (distilbert-base-uncased-finetuned-sst-2-english) on the test set: {'eval_loss': 0.4226415455341339, 'eval_accuracy': 0.9141333333333334, 'eval_f1': 0.914133235636037, 'eval_runtime': 62.0033, 'eval_samples_per_second': 120.961, 'eval_steps_per_second': 15.128, 'epoch': 3.0}\nFine-tuned model distilbert-base-uncased-finetuned-sst-2-english has been saved to /kaggle/working/distilbert_finetuned\n\nClassification Report (fine-tuned distilbert-base-uncased-finetuned-sst-2-english):\n              precision    recall  f1-score   support\n\n           0      0.915     0.913     0.914      3750\n           1      0.913     0.915     0.914      3750\n\n    accuracy                          0.914      7500\n   macro avg      0.914     0.914     0.914      7500\nweighted avg      0.914     0.914     0.914      7500\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 2. Twitter RoBERTa","metadata":{}},{"cell_type":"code","source":"print(\"\\nTwitter RoBERTa\")\n# Tokenize data for Twitter RoBERTa\ntwitter_roberta_tokenizer = twitter_roberta_config['tokenizer_class'].from_pretrained(twitter_roberta_config['model_name'])\ntwitter_roberta_tokenized_datasets = raw_datasets.map(lambda x: tokenize_function(x, twitter_roberta_tokenizer), batched=True)\ntwitter_roberta_tokenized_datasets = twitter_roberta_tokenized_datasets.remove_columns(['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T10:01:37.183884Z","iopub.execute_input":"2025-08-24T10:01:37.184217Z","iopub.status.idle":"2025-08-24T10:01:55.418708Z","shell.execute_reply.started":"2025-08-24T10:01:37.184185Z","shell.execute_reply":"2025-08-24T10:01:55.418179Z"}},"outputs":[{"name":"stdout","text":"\nTwitter RoBERTa\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152e728b9a1f4a3d83acc7ee8a2250e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5225fffd4bb4827871bf9128bbd8424"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b4175dfe86412d939c23ec1e92f7fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f95d5ce2219540719dd868f392c32579"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dcf603a242b41998dbc2e233335404c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d41aab3744443da83a772969e119194"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66bead0d72c548939fa377755fa4cd7a"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Fine-tuning Twitter RoBERTa\nfine_tune_model(\n    twitter_roberta_config['model_name'],\n    twitter_roberta_config['tokenizer_class'],\n    twitter_roberta_config['model_class'],\n    twitter_roberta_config['output_dir'],\n    twitter_roberta_tokenized_datasets\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T10:01:55.419943Z","iopub.execute_input":"2025-08-24T10:01:55.420190Z","iopub.status.idle":"2025-08-24T11:46:34.607893Z","shell.execute_reply.started":"2025-08-24T10:01:55.420164Z","shell.execute_reply":"2025-08-24T11:46:34.607253Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fbdfdbf559b4137ac5d8775c70f6c21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ede824085ff24119b1337015902a2b00"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13125' max='13125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13125/13125 1:38:36, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.346700</td>\n      <td>0.333933</td>\n      <td>0.894533</td>\n      <td>0.894532</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.268700</td>\n      <td>0.330771</td>\n      <td>0.904933</td>\n      <td>0.904927</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.183800</td>\n      <td>0.391748</td>\n      <td>0.909467</td>\n      <td>0.909415</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Evaluation results of the fine-tuned model (cardiffnlp/twitter-roberta-base-sentiment-latest) on the validation set: {'eval_loss': 0.3917475640773773, 'eval_accuracy': 0.9094666666666666, 'eval_f1': 0.9094150678789316, 'eval_runtime': 118.7401, 'eval_samples_per_second': 63.163, 'eval_steps_per_second': 7.9, 'epoch': 3.0}\nEvaluation results of the fine-tuned model (cardiffnlp/twitter-roberta-base-sentiment-latest) on the test set: {'eval_loss': 0.3520101010799408, 'eval_accuracy': 0.9194666666666667, 'eval_f1': 0.9194480559455123, 'eval_runtime': 118.6957, 'eval_samples_per_second': 63.187, 'eval_steps_per_second': 7.903, 'epoch': 3.0}\nFine-tuned model cardiffnlp/twitter-roberta-base-sentiment-latest has been saved to /kaggle/working/twitter_roberta_finetuned\n\nClassification Report (fine-tuned cardiffnlp/twitter-roberta-base-sentiment-latest):\n              precision    recall  f1-score   support\n\n           0      0.907     0.935     0.921      3750\n           1      0.933     0.904     0.918      3750\n\n    accuracy                          0.919      7500\n   macro avg      0.920     0.919     0.919      7500\nweighted avg      0.920     0.919     0.919      7500\n\n","output_type":"stream"}],"execution_count":13}]}