{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12543401,"sourceType":"datasetVersion","datasetId":7919180}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate\n\nimport pandas as pd\nimport numpy as np\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    DistilBertTokenizer, DistilBertForSequenceClassification,\n    RobertaTokenizer, RobertaForSequenceClassification,\n    Trainer, TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport evaluate\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:31:04.598563Z","iopub.execute_input":"2025-07-31T13:31:04.599106Z","iopub.status.idle":"2025-07-31T13:31:49.115235Z","shell.execute_reply.started":"2025-07-31T13:31:04.599075Z","shell.execute_reply":"2025-07-31T13:31:49.114634Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2025.3.0\n","output_type":"stream"},{"name":"stderr","text":"2025-07-31 13:31:30.692104: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753968691.065047      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753968691.168845      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Đọc và xử lý dữ liệu\ndata = pd.read_csv('/kaggle/input/pretrained-data/preprocessed_results.csv')\ndata['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:31:49.116382Z","iopub.execute_input":"2025-07-31T13:31:49.116926Z","iopub.status.idle":"2025-07-31T13:31:50.170350Z","shell.execute_reply.started":"2025-07-31T13:31:49.116902Z","shell.execute_reply":"2025-07-31T13:31:50.169801Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Chia tập train, validation, test\ntrain_data, temp_data = train_test_split(data, test_size=0.3, random_state=42, stratify=data['sentiment'])\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['sentiment'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:31:50.171400Z","iopub.execute_input":"2025-07-31T13:31:50.171703Z","iopub.status.idle":"2025-07-31T13:31:50.208857Z","shell.execute_reply.started":"2025-07-31T13:31:50.171678Z","shell.execute_reply":"2025-07-31T13:31:50.208352Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Lưu các tập dữ liệu vào CSV\nos.makedirs('/kaggle/working/datasets', exist_ok=True)\ntrain_data.to_csv('/kaggle/working/datasets/train_data.csv', index=False)\nval_data.to_csv('/kaggle/working/datasets/val_data.csv', index=False)\ntest_data.to_csv('/kaggle/working/datasets/test_data.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:31:50.210312Z","iopub.execute_input":"2025-07-31T13:31:50.210696Z","iopub.status.idle":"2025-07-31T13:31:51.313481Z","shell.execute_reply.started":"2025-07-31T13:31:50.210678Z","shell.execute_reply":"2025-07-31T13:31:51.312735Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Chuyển thành DatasetDict\nraw_datasets = DatasetDict({\n    'train': Dataset.from_pandas(train_data[['preprocessed_tokens', 'sentiment']].rename(columns={'preprocessed_tokens': 'text', 'sentiment': 'labels'})),\n    'valid': Dataset.from_pandas(val_data[['preprocessed_tokens', 'sentiment']].rename(columns={'preprocessed_tokens': 'text', 'sentiment': 'labels'})),\n    'test': Dataset.from_pandas(test_data[['preprocessed_tokens', 'sentiment']].rename(columns={'preprocessed_tokens': 'text', 'sentiment': 'labels'})),\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:31:51.314219Z","iopub.execute_input":"2025-07-31T13:31:51.314700Z","iopub.status.idle":"2025-07-31T13:31:51.653227Z","shell.execute_reply.started":"2025-07-31T13:31:51.314675Z","shell.execute_reply":"2025-07-31T13:31:51.652446Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Định nghĩa hàm compute_metrics\ndef compute_metrics(eval_pred):\n    metric_acc = evaluate.load(\"accuracy\")\n    metric_f1 = evaluate.load(\"f1\")\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    acc = metric_acc.compute(predictions=predictions, references=labels)\n    f1 = metric_f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:31:51.654078Z","iopub.execute_input":"2025-07-31T13:31:51.654330Z","iopub.status.idle":"2025-07-31T13:31:51.658897Z","shell.execute_reply.started":"2025-07-31T13:31:51.654308Z","shell.execute_reply":"2025-07-31T13:31:51.658220Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Hàm để đánh giá mô hình trước khi fine-tune\ndef evaluate_pre_finetuned_model(model_name, tokenizer_class, model_class, tokenized_datasets):\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n    \n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=256)\n    \n    trainer = Trainer(\n        model=model,\n        args=TrainingArguments(\n            output_dir=f\"/kaggle/working/{model_name.split('/')[-1]}_prefinetuned\",\n            eval_strategy=\"epoch\",\n            report_to=\"none\"\n        ),\n        eval_dataset=tokenized_datasets['test'],\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n    \n    # Đánh giá trên tập test\n    test_results = trainer.evaluate()\n    print(f\"Kết quả đánh giá pre-fine-tuned model ({model_name}) trên tập test: {test_results}\")\n    \n    # Dự đoán để tạo classification report\n    y_pred = trainer.predict(tokenized_datasets['test']).predictions\n    y_pred = np.argmax(y_pred, axis=-1)\n    y_true = tokenized_datasets['test']['labels']\n    print(f\"\\nClassification Report pre-fine-tuned ({model_name}):\")\n    print(classification_report(y_true, y_pred, digits=3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:31:51.659814Z","iopub.execute_input":"2025-07-31T13:31:51.660047Z","iopub.status.idle":"2025-07-31T13:31:51.681832Z","shell.execute_reply.started":"2025-07-31T13:31:51.660025Z","shell.execute_reply":"2025-07-31T13:31:51.681091Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# # Hàm để fine-tune mô hình\n# def fine_tune_model(model_name, tokenizer_class, model_class, output_dir):\n#     # Khởi tạo tokenizer và model\n#     tokenizer = tokenizer_class.from_pretrained(model_name)\n#     model = model_class.from_pretrained(model_name, num_labels=2)\n\n#     # Tokenize dữ liệu\n#     def tokenize_function(examples):\n#         return tokenizer(examples['text'], truncation=True)\n\n#     tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n#     tokenized_datasets = tokenized_datasets.remove_columns(['text'])\n\n#     # Thiết lập DataCollator\n#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n#     # Thiết lập TrainingArguments\n#     training_args = TrainingArguments(\n#         output_dir=output_dir,\n#         num_train_epochs=3,\n#         eval_strategy=\"epoch\",\n#         weight_decay=5e-4,\n#         optim=\"adamw_torch\",\n#         learning_rate=5e-5,\n#         save_strategy=\"no\",\n#         fp16=True,\n#         push_to_hub=False,\n#         report_to=\"none\"\n#     )\n\n#     # Khởi tạo Trainer\n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=tokenized_datasets['train'],\n#         eval_dataset=tokenized_datasets['valid'],\n#         data_collator=data_collator,\n#         tokenizer=tokenizer,\n#         compute_metrics=compute_metrics\n#     )\n\n#     # Huấn luyện mô hình\n#     trainer.train()\n\n#     # Đánh giá trên tập validation\n#     val_results = trainer.evaluate()\n#     print(f\"Kết quả đánh giá trên tập validation ({model_name}): {val_results}\")\n\n#     # Đánh giá trên tập test\n#     test_results = trainer.evaluate(tokenized_datasets['test'])\n#     print(f\"Kết quả đánh giá trên tập test ({model_name}): {test_results}\")\n\n#     # Lưu mô hình\n#     model.save_pretrained(output_dir)\n#     tokenizer.save_pretrained(output_dir)\n#     print(f\"Mô hình {model_name} đã được lưu tại {output_dir}\")\n\n#     # Dự đoán trên tập test để có classification report\n#     y_pred = trainer.predict(tokenized_datasets['test']).predictions\n#     y_pred = np.argmax(y_pred, axis=-1)\n#     y_true = tokenized_datasets['test']['labels']\n#     print(f\"\\nClassification Report ({model_name}):\")\n#     print(classification_report(y_true, y_pred, digits=3))\n\n# Hàm để fine-tune và đánh giá mô hình\ndef fine_tune_model(model_name, tokenizer_class, model_class, output_dir, tokenized_datasets):\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=256)\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=3,\n        eval_strategy=\"epoch\",\n        weight_decay=5e-4,\n        optim=\"adamw_torch\",\n        learning_rate=5e-5,\n        save_strategy=\"no\",\n        fp16=True,\n        push_to_hub=False,\n        report_to=\"none\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets['train'],\n        eval_dataset=tokenized_datasets['valid'],\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n\n    # Huấn luyện mô hình\n    trainer.train()\n\n    # Đánh giá trên tập validation\n    val_results = trainer.evaluate()\n    print(f\"Kết quả đánh giá fine-tuned model ({model_name}) trên tập validation: {val_results}\")\n\n    # Đánh giá trên tập test\n    test_results = trainer.evaluate(tokenized_datasets['test'])\n    print(f\"Kết quả đánh giá fine-tuned model ({model_name}) trên tập test: {test_results}\")\n\n    # Lưu mô hình\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Mô hình fine-tuned {model_name} đã được lưu tại {output_dir}\")\n\n    # Dự đoán để tạo classification report\n    y_pred = trainer.predict(tokenized_datasets['test']).predictions\n    y_pred = np.argmax(y_pred, axis=-1)\n    y_true = tokenized_datasets['test']['labels']\n    print(f\"\\nClassification Report fine-tuned ({model_name}):\")\n    print(classification_report(y_true, y_pred, digits=3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:31:51.682678Z","iopub.execute_input":"2025-07-31T13:31:51.682924Z","iopub.status.idle":"2025-07-31T13:31:51.701705Z","shell.execute_reply.started":"2025-07-31T13:31:51.682902Z","shell.execute_reply":"2025-07-31T13:31:51.701093Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Tokenize dữ liệu cho cả hai mô hình\ndef tokenize_function(examples, tokenizer):\n    # return tokenizer(examples['text'], truncation=True)\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512, return_token_type_ids=False)\n\n# Cấu hình mô hình DistilBERT\ndistilbert_config = {\n    'model_name': \"distilbert-base-uncased-finetuned-sst-2-english\",\n    'tokenizer_class': DistilBertTokenizer,\n    'model_class': DistilBertForSequenceClassification,\n    'output_dir': \"/kaggle/working/distilbert_finetuned\"\n}\n\n# Cấu hình mô hình RoBERTa\ntwitter_roberta_config = {\n    'model_name': \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    'tokenizer_class': AutoTokenizer,\n    'model_class': AutoModelForSequenceClassification,\n    'output_dir': \"/kaggle/working/twitter_roberta_finetuned\"\n}\n\n# # Tokenize dữ liệu cho từng mô hình\n# for config in [distilbert_config, roberta_config]:\n#     tokenizer = config['tokenizer_class'].from_pretrained(config['model_name'])\n#     tokenized_datasets = raw_datasets.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n#     tokenized_datasets = tokenized_datasets.remove_columns(['text'])\n    \n#     print(f\"\\nĐánh giá pre-fine-tuned model: {config['model_name']}\")\n#     evaluate_pre_finetuned_model(config['model_name'], config['tokenizer_class'], config['model_class'], tokenized_datasets)\n    \n#     print(f\"\\nFine-tuning model: {config['model_name']}\")\n#     fine_tune_model(config['model_name'], config['tokenizer_class'], config['model_class'], config['output_dir'], tokenized_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:31:51.702280Z","iopub.execute_input":"2025-07-31T13:31:51.702463Z","iopub.status.idle":"2025-07-31T13:31:51.725386Z","shell.execute_reply.started":"2025-07-31T13:31:51.702448Z","shell.execute_reply":"2025-07-31T13:31:51.724895Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Xử lý DistilBERT\nprint(\"\\n=== Xử lý DistilBERT ===\")\n# Tokenize dữ liệu cho DistilBERT\ndistilbert_tokenizer = distilbert_config['tokenizer_class'].from_pretrained(distilbert_config['model_name'])\ndistilbert_tokenized_datasets = raw_datasets.map(lambda x: tokenize_function(x, distilbert_tokenizer), batched=True)\ndistilbert_tokenized_datasets = distilbert_tokenized_datasets.remove_columns(['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:31:51.727247Z","iopub.execute_input":"2025-07-31T13:31:51.727696Z","iopub.status.idle":"2025-07-31T13:33:45.195745Z","shell.execute_reply.started":"2025-07-31T13:31:51.727671Z","shell.execute_reply":"2025-07-31T13:33:45.195215Z"}},"outputs":[{"name":"stdout","text":"\n=== Xử lý DistilBERT ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b363672b6be48f681440c59074a0edd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a9662ec9ae7439b978554bba976055e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01be57e335a1450b86ca45f21e7e6d2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac319e3d683d4d6cbff4f20a57eb5ed1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"079ee1b672934089a43308910b005d05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57f6f2adbcf04faab7fef4998566873e"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Đánh giá pre-fine-tuned DistilBERT\nprint(\"\\nĐánh giá pre-fine-tuned model: distilbert-base-uncased-finetuned-sst-2-english\")\nevaluate_pre_finetuned_model(\n    distilbert_config['model_name'],\n    distilbert_config['tokenizer_class'],\n    distilbert_config['model_class'],\n    distilbert_tokenized_datasets\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:33:45.196496Z","iopub.execute_input":"2025-07-31T13:33:45.196769Z","iopub.status.idle":"2025-07-31T13:36:00.597534Z","shell.execute_reply.started":"2025-07-31T13:33:45.196744Z","shell.execute_reply":"2025-07-31T13:36:00.596649Z"}},"outputs":[{"name":"stdout","text":"\nĐánh giá pre-fine-tuned model: distilbert-base-uncased-finetuned-sst-2-english\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1058df6130eb457caac6f437f885a945"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/1161746956.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eec9cca89a284150bbdc0ca59faf7c84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8894005c89c04926b2ced76b5c54de4f"}},"metadata":{}},{"name":"stdout","text":"Kết quả đánh giá pre-fine-tuned model (distilbert-base-uncased-finetuned-sst-2-english) trên tập test: {'eval_loss': 0.7170329689979553, 'eval_accuracy': 0.7956, 'eval_f1': 0.7919230151894299, 'eval_runtime': 64.5374, 'eval_samples_per_second': 116.212, 'eval_steps_per_second': 7.267}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report pre-fine-tuned (distilbert-base-uncased-finetuned-sst-2-english):\n              precision    recall  f1-score   support\n\n           0      0.734     0.929     0.820      3750\n           1      0.903     0.663     0.764      3750\n\n    accuracy                          0.796      7500\n   macro avg      0.818     0.796     0.792      7500\nweighted avg      0.818     0.796     0.792      7500\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Fine-tuning DistilBERT\nprint(\"\\nFine-tuning model: distilbert-base-uncased-finetuned-sst-2-english\")\nfine_tune_model(\n    distilbert_config['model_name'],\n    distilbert_config['tokenizer_class'],\n    distilbert_config['model_class'],\n    distilbert_config['output_dir'],\n    distilbert_tokenized_datasets\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T13:36:00.598374Z","iopub.execute_input":"2025-07-31T13:36:00.598675Z","iopub.status.idle":"2025-07-31T14:32:19.795485Z","shell.execute_reply.started":"2025-07-31T13:36:00.598647Z","shell.execute_reply":"2025-07-31T14:32:19.794740Z"}},"outputs":[{"name":"stdout","text":"\nFine-tuning model: distilbert-base-uncased-finetuned-sst-2-english\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3317148887.py:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6564' max='6564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6564/6564 52:50, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.272300</td>\n      <td>0.250442</td>\n      <td>0.905067</td>\n      <td>0.905063</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.356700</td>\n      <td>0.947509</td>\n      <td>0.902133</td>\n      <td>0.902126</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.246700</td>\n      <td>1.401354</td>\n      <td>0.906267</td>\n      <td>0.906265</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Kết quả đánh giá fine-tuned model (distilbert-base-uncased-finetuned-sst-2-english) trên tập validation: {'eval_loss': 1.4013543128967285, 'eval_accuracy': 0.9062666666666667, 'eval_f1': 0.9062648519542005, 'eval_runtime': 68.5727, 'eval_samples_per_second': 109.373, 'eval_steps_per_second': 6.839, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Kết quả đánh giá fine-tuned model (distilbert-base-uncased-finetuned-sst-2-english) trên tập test: {'eval_loss': 1.3239593505859375, 'eval_accuracy': 0.9113333333333333, 'eval_f1': 0.9113332560947476, 'eval_runtime': 68.4343, 'eval_samples_per_second': 109.594, 'eval_steps_per_second': 6.853, 'epoch': 3.0}\nMô hình fine-tuned distilbert-base-uncased-finetuned-sst-2-english đã được lưu tại /kaggle/working/distilbert_finetuned\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report fine-tuned (distilbert-base-uncased-finetuned-sst-2-english):\n              precision    recall  f1-score   support\n\n           0      0.912     0.910     0.911      3750\n           1      0.911     0.912     0.911      3750\n\n    accuracy                          0.911      7500\n   macro avg      0.911     0.911     0.911      7500\nweighted avg      0.911     0.911     0.911      7500\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Xử lý Twitter RoBERTa\nprint(\"\\n=== Xử lý Twitter RoBERTa ===\")\n# Tokenize dữ liệu cho Twitter RoBERTa\ntwitter_roberta_tokenizer = twitter_roberta_config['tokenizer_class'].from_pretrained(twitter_roberta_config['model_name'])\ntwitter_roberta_tokenized_datasets = raw_datasets.map(lambda x: tokenize_function(x, twitter_roberta_tokenizer), batched=True)\ntwitter_roberta_tokenized_datasets = twitter_roberta_tokenized_datasets.remove_columns(['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T14:32:19.796482Z","iopub.execute_input":"2025-07-31T14:32:19.796752Z","iopub.status.idle":"2025-07-31T14:32:38.724494Z","shell.execute_reply.started":"2025-07-31T14:32:19.796727Z","shell.execute_reply":"2025-07-31T14:32:38.723968Z"}},"outputs":[{"name":"stdout","text":"\n=== Xử lý Twitter RoBERTa ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c906541580b1412ba412d67853002d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94ae85e087774fb0be6a2acbb5af7790"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf27d15f1f44147aff233c7c4fe77d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0672c63283004a23abba7597270f0638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f3c7181d6794b67815cb0e1d7d4f719"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dff18c9edad4c28ac0b51080bb32836"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4734f7374ef4055a6421cb3fc2a44ed"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Đánh giá pre-fine-tuned Twitter RoBERTa\nprint(\"\\nĐánh giá pre-fine-tuned model: cardiffnlp/twitter-roberta-base-sentiment-latest\")\nevaluate_pre_finetuned_model(\n    twitter_roberta_config['model_name'],\n    twitter_roberta_config['tokenizer_class'],\n    twitter_roberta_config['model_class'],\n    twitter_roberta_tokenized_datasets\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T14:32:38.725215Z","iopub.execute_input":"2025-07-31T14:32:38.725453Z","iopub.status.idle":"2025-07-31T14:37:00.595356Z","shell.execute_reply.started":"2025-07-31T14:32:38.725431Z","shell.execute_reply":"2025-07-31T14:37:00.594565Z"}},"outputs":[{"name":"stdout","text":"\nĐánh giá pre-fine-tuned model: cardiffnlp/twitter-roberta-base-sentiment-latest\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c8e8f442454f9b91e59a1a5f1dce6f"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/1161746956.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28a2238dca754d5e9178f1d53cf2ca80"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Kết quả đánh giá pre-fine-tuned model (cardiffnlp/twitter-roberta-base-sentiment-latest) trên tập test: {'eval_loss': 0.7688709497451782, 'eval_accuracy': 0.2976, 'eval_f1': 0.26086555692611063, 'eval_runtime': 128.9806, 'eval_samples_per_second': 58.148, 'eval_steps_per_second': 3.636}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report pre-fine-tuned (cardiffnlp/twitter-roberta-base-sentiment-latest):\n              precision    recall  f1-score   support\n\n           0      0.135     0.075     0.096      3750\n           1      0.360     0.521     0.426      3750\n\n    accuracy                          0.298      7500\n   macro avg      0.247     0.298     0.261      7500\nweighted avg      0.247     0.298     0.261      7500\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Fine-tuning Twitter RoBERTa\nprint(\"\\nFine-tuning model: cardiffnlp/twitter-roberta-base-sentiment-latest\")\nfine_tune_model(\n    twitter_roberta_config['model_name'],\n    twitter_roberta_config['tokenizer_class'],\n    twitter_roberta_config['model_class'],\n    twitter_roberta_config['output_dir'],\n    twitter_roberta_tokenized_datasets\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T14:37:00.596110Z","iopub.execute_input":"2025-07-31T14:37:00.596395Z","iopub.status.idle":"2025-07-31T16:27:08.092242Z","shell.execute_reply.started":"2025-07-31T14:37:00.596362Z","shell.execute_reply":"2025-07-31T16:27:08.091641Z"}},"outputs":[{"name":"stdout","text":"\nFine-tuning model: cardiffnlp/twitter-roberta-base-sentiment-latest\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/3317148887.py:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6564' max='6564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6564/6564 1:43:42, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.288700</td>\n      <td>0.288155</td>\n      <td>0.901333</td>\n      <td>0.901302</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.270700</td>\n      <td>0.351296</td>\n      <td>0.911200</td>\n      <td>0.911192</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.420200</td>\n      <td>1.190983</td>\n      <td>0.916400</td>\n      <td>0.916389</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Kết quả đánh giá fine-tuned model (cardiffnlp/twitter-roberta-base-sentiment-latest) trên tập validation: {'eval_loss': 1.1909832954406738, 'eval_accuracy': 0.9164, 'eval_f1': 0.9163892606650366, 'eval_runtime': 126.8, 'eval_samples_per_second': 59.148, 'eval_steps_per_second': 3.699, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Kết quả đánh giá fine-tuned model (cardiffnlp/twitter-roberta-base-sentiment-latest) trên tập test: {'eval_loss': 1.086658239364624, 'eval_accuracy': 0.9237333333333333, 'eval_f1': 0.9237321130471421, 'eval_runtime': 126.9619, 'eval_samples_per_second': 59.073, 'eval_steps_per_second': 3.694, 'epoch': 3.0}\nMô hình fine-tuned cardiffnlp/twitter-roberta-base-sentiment-latest đã được lưu tại /kaggle/working/twitter_roberta_finetuned\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report fine-tuned (cardiffnlp/twitter-roberta-base-sentiment-latest):\n              precision    recall  f1-score   support\n\n           0      0.920     0.928     0.924      3750\n           1      0.927     0.920     0.923      3750\n\n    accuracy                          0.924      7500\n   macro avg      0.924     0.924     0.924      7500\nweighted avg      0.924     0.924     0.924      7500\n\n","output_type":"stream"}],"execution_count":15}]}